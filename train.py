# íŒŒì¼ëª…: model.py
# (CUDA 'device-side assert' ì˜¤ë¥˜ ìˆ˜ì • ì™„ë£Œ)

import os
import torch
import easyocr
import numpy as np
from PIL import Image
from ultralytics import YOLO
# ğŸ‘‡ 'AutoConfig'ë¥¼ ì¶”ê°€ë¡œ ì„í¬íŠ¸í•©ë‹ˆë‹¤.
from transformers import CLIPModel, CLIPProcessor, CLIPImageProcessor, AutoTokenizer, AutoConfig

class QueryBasedDetector:
    def __init__(self, best_pt_path, device=None):
        """
        ì¶”ë¡ ì— í•„ìš”í•œ ëª¨ë“  ëª¨ë¸(YOLO, CLIP, OCR)ì„
        í•œ ë²ˆë§Œ ë¡œë“œí•˜ì—¬ í´ë˜ìŠ¤ì— ì €ì¥í•©ë‹ˆë‹¤.
        
        Args:
            best_pt_path (str): train.pyë¡œ í•™ìŠµì‹œí‚¨ 'best.pt' íŒŒì¼ ê²½ë¡œ
            device (str, optional): 'cuda' ë˜ëŠ” 'cpu'. Noneì´ë©´ ìë™ ê°ì§€.
        """
        
        if device is None:
            self.device = "cuda" if torch.cuda.is_available() else "cpu"
        else:
            self.device = device
            
        print(f"Using device: {self.device}")

        # 1. (Stage 1) YOLO íƒì§€ê¸° ë¡œë“œ
        self.detector = YOLO(best_pt_path).to(self.device)
        print(f"YOLO detector loaded from: {best_pt_path}")

        # --- ğŸ‘‡ [ìˆ˜ì •ë¨] CLIP ë¡œë” ë¡œì§ (CUDA ì˜¤ë¥˜ í•´ê²°) ---
        
        # 2. (Stage 2) CLIP ë§¤ì¹­ê¸° ë¡œë“œ
        clip_model_name = "sentence-transformers/clip-ViT-B-32-multilingual-v1"
        base_clip_name = "openai/clip-vit-base-patch32" # ì´ë¯¸ì§€ í”„ë¡œì„¸ì„œ ì°¸ì¡°ìš©
        
        print(f"Loading CLIP model: {clip_model_name}")

        try:
            # (a) ë¡œë“œí•  ëª¨ë¸ì˜ 'ì„¤ì •(Config)'ì„ ëª…ì‹œì ìœ¼ë¡œ ë¶ˆëŸ¬ì˜µë‹ˆë‹¤.
            # ì´ configì—ëŠ” ì˜¬ë°”ë¥¸ vocab_size(119547)ê°€ í¬í•¨ë˜ì–´ ìˆìŠµë‹ˆë‹¤.
            config = AutoConfig.from_pretrained(clip_model_name)
            
            # (b) ëª¨ë¸ ê°€ì¤‘ì¹˜ ë¡œë“œ (ì¤‘ìš”: config=config ì „ë‹¬)
            # ëª¨ë¸ì´ ìƒì„±ë  ë•Œ ìœ„ì—ì„œ ë¡œë“œí•œ configë¥¼ ì‚¬ìš©í•˜ë„ë¡ ê°•ì œí•©ë‹ˆë‹¤.
            self.clip_model = CLIPModel.from_pretrained(
                clip_model_name,
                config=config 
            ).to(self.device)
            
            # (c) í…ìŠ¤íŠ¸ í† í¬ë‚˜ì´ì € ë¡œë“œ (ë‹¤êµ­ì–´)
            tokenizer = AutoTokenizer.from_pretrained(clip_model_name)
            
            # (d) ì´ë¯¸ì§€ í”„ë¡œì„¸ì„œ ë¡œë“œ (ì›ë³¸)
            image_processor = CLIPImageProcessor.from_pretrained(base_clip_name)

            # (e) CLIPProcessor ìˆ˜ë™ ì¡°í•©
            self.clip_processor = CLIPProcessor(image_processor=image_processor, tokenizer=tokenizer)
            
            print(f"CLIP model and processor loaded successfully.")
            
        except Exception as e:
            print(f"Error loading CLIP components: {e}")
            raise e # ì˜¤ë¥˜ ë°œìƒ ì‹œ ì¤‘ì§€

        # --- ğŸ‘† [ìˆ˜ì •] ì—¬ê¸°ê¹Œì§€ ---

        # 3. (Stage 2) OCR ë¡œë“œ (ìº¡ì…˜ ì¶”ì¶œìš©)
        self.ocr_reader = easyocr.Reader(['ko', 'en'], gpu=(self.device == "cuda"))
        print("EasyOCR loaded.")

        # 4. ìŠ¤ì½”ì–´ë§ ê°€ì¤‘ì¹˜ (ì‹¤í—˜ì ìœ¼ë¡œ ì¡°ì ˆ)
        self.caption_weight = 0.7
        self.visual_weight = 0.3

    @torch.no_grad() # ì¶”ë¡  ëª¨ë“œì—ì„œëŠ” ê·¸ë˜ë””ì–¸íŠ¸ ê³„ì‚° ë¹„í™œì„±í™”
    def predict(self, image_path, query_text):
        """
        í•˜ë‚˜ì˜ ì´ë¯¸ì§€ì™€ ì§ˆë¬¸(query)ì„ ë°›ì•„
        ê°€ì¥ ì ìˆ˜ê°€ ë†’ì€ ê°ì²´ì˜ [x, y, w, h]ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.
        """
        
        try:
            # ì¿¼ë¦¬ í…ìŠ¤íŠ¸ê°€ Noneì´ê±°ë‚˜ float(NaN)ì¼ ê²½ìš°ë¥¼ ëŒ€ë¹„
            if not isinstance(query_text, str):
                query_text = "" # ë¹ˆ ë¬¸ìì—´ë¡œ ì²˜ë¦¬

            image_pil = Image.open(image_path).convert("RGB")
        except Exception as e:
            print(f"Error opening image {image_path}: {e}")
            return [0, 0, 0, 0] 

        # [ìˆ˜ì •] YOLO conf ê°’ ì¡°ì ˆë¡œ í›„ë³´êµ° í™•ëŒ€
        yolo_results = self.detector.predict(image_pil, verbose=False, conf=0.1) 
        candidate_boxes = yolo_results[0].boxes.xyxy.cpu().numpy() 

        if len(candidate_boxes) == 0:
            return [0, 0, 0, 0] 

        # 2-1. ì§ˆë¬¸(Query) í…ìŠ¤íŠ¸ë¥¼ CLIP í”¼ì²˜ë¡œ ë³€í™˜ (í•œ ë²ˆë§Œ)
        query_inputs = self.clip_processor(text=[query_text], return_tensors="pt", padding=True, truncation=True).to(self.device)
        
        # (ì˜¤ë¥˜ê°€ ë°œìƒí–ˆë˜ ì§€ì )
        query_features = self.clip_model.get_text_features(**query_inputs).detach() # Shape: [1, D]

        best_box = None
        highest_score = -float('inf')

        for box in candidate_boxes:
            x1, y1, x2, y2 = map(int, box)

            # 1. ìº¡ì…˜ 3ê°œ ì¶”ì¶œ (ìœ„/ì•„ë˜)
            caption_text_above = self._get_caption_text(image_pil, box, "above")
            caption_text_below = self._get_caption_text(image_pil, box, "below")
            
            # 2-3. ì´ë¯¸ì§€ ì¡°ê°(patch) ì¶”ì¶œ
            patch_img = image_pil.crop((x1, y1, x2, y2))
            
            # (a) ì´ë¯¸ì§€ ì¸ì½”ë”©
            image_inputs = self.clip_processor(images=[patch_img], return_tensors="pt").to(self.device)
            visual_features = self.clip_model.get_image_features(image_inputs.pixel_values).detach() # Shape: [1, D]
            
            # (b) í…ìŠ¤íŠ¸ ì¸ì½”ë”© (ìœ„/ì•„ë˜)
            text_inputs = self.clip_processor(
                text=[caption_text_above, caption_text_below],
                return_tensors="pt", 
                padding=True,
                truncation=True
            ).to(self.device)
            text_features = self.clip_model.get_text_features(text_inputs.input_ids, text_inputs.attention_mask).detach() # Shape: [2, D]
            caption_features_above = text_features[0] # Shape: [D]
            caption_features_below = text_features[1] # Shape: [D]

            # --- [ìˆ˜ì •ë¨] ì ì‘í˜• ìŠ¤ì½”ì–´ë§ ë¡œì§ ---

            # (a) ì´ë¯¸ì§€ ì ìˆ˜ (ê³µí†µ)
            score_visual = torch.nn.functional.cosine_similarity(query_features, visual_features)

            # (b) í…ìŠ¤íŠ¸ ì ìˆ˜ (OCR ê²°ê³¼ê°€ ìˆì„ ë•Œë§Œ ê³„ì‚°)
            has_caption_above = len(caption_text_above.strip()) > 0
            has_caption_below = len(caption_text_below.strip()) > 0

            score_caption = 0.0 # ê¸°ë³¸ê°’

            if has_caption_above or has_caption_below:
                score_caption_above = 0.0
                score_caption_below = 0.0
                
                if has_caption_above:
                    score_caption_above = torch.nn.functional.cosine_similarity(query_features, caption_features_above.unsqueeze(0))
                if has_caption_below:
                    score_caption_below = torch.nn.functional.cosine_similarity(query_features, caption_features_below.unsqueeze(0))
                
                score_caption = max(score_caption_above, score_caption_below)
                
                # ìº¡ì…˜ì´ ìˆì„ ë•Œì˜ ìµœì¢… ì ìˆ˜
                final_score = (self.caption_weight * score_caption) + (self.visual_weight * score_visual)
            
            else:
                # ìº¡ì…˜ì´ ì•„ì˜ˆ ì—†ìœ¼ë©´ ë¹„ì£¼ì–¼ ì ìˆ˜ë§Œ 100% ë°˜ì˜
                final_score = score_visual 
            
            # --- ğŸ‘† ---
            
            if final_score > highest_score:
                highest_score = final_score
                best_box = box 

        # 3. í¬ë§· ë³€í™˜ ë° ë°˜í™˜
        if best_box is not None:
            x1, y1, x2, y2 = best_box
            pred_x = x1
            pred_y = y1
            pred_w = x2 - x1
            pred_h = y2 - y1
            return [float(pred_x), float(pred_y), float(pred_w), float(pred_h)]
        else:
            return [0, 0, 0, 0] 

    # --- [ìˆ˜ì •ë¨] OCR ì˜ì—­ í™•ì¥ (Xì¶•) ---
    def _get_caption_text(self, image_pil, box, position="below", margin_px=50, x_expand_px=100):
        """
        (Helper) BBoxì˜ 'ìœ„(above)' ë˜ëŠ” 'ì•„ë˜(below)'ì—ì„œ í…ìŠ¤íŠ¸ë¥¼ OCRë¡œ ì¶”ì¶œ
        (x_expand_pxë¥¼ ì¶”ê°€í•˜ì—¬ BBox ë„ˆë¹„ë³´ë‹¤ ë„“ê²Œ íƒìƒ‰)
        """
        try:
            x1, y1, x2, y2 = map(int, box)
            img_width, img_height = image_pil.size
            
            # BBox ë„ˆë¹„ë³´ë‹¤ ì¢Œìš° 100pxì”© ë„“ê²Œ íƒìƒ‰
            cap_x1 = max(0, x1 - x_expand_px)
            cap_x2 = min(img_width, x2 + x_expand_px)
            
            cap_y1, cap_y2 = 0, 0

            if position == "above":
                cap_y2 = max(0, y1)            
                cap_y1 = max(0, y1 - margin_px)
            else: # "below" (default)
                cap_y1 = min(y2, img_height)      
                cap_y2 = min(img_height, y2 + margin_px) 

            if cap_x1 >= cap_x2 or cap_y1 >= cap_y2:
                return "" # ì˜ì—­ì´ ì—†ìŒ

            caption_zone_img = image_pil.crop((cap_x1, cap_y1, cap_x2, cap_y2))
            
            ocr_results = self.ocr_reader.readtext(np.array(caption_zone_img), detail=0)
            return " ".join(ocr_results)
        except Exception as e:
            # OCR ì—ëŸ¬ê°€ ë‚˜ë„ ë¹ˆ ë¬¸ìì—´ì„ ë°˜í™˜í•˜ì—¬ ë©”ì¸ ë¡œì§ì´ ë©ˆì¶”ì§€ ì•Šê²Œ í•¨
            # print(f"Warning: OCR failed for box {box} ({position}). Error: {e}")
            return ""
    # --- ğŸ‘† ---

# ---
# ì´ íŒŒì¼ì´ ì§ì ‘ ì‹¤í–‰ë  ë•Œ (í…ŒìŠ¤íŠ¸ìš©)
if __name__ == "__main__":
    # train.pyì—ì„œ ìƒì„±ëœ 'best.pt' ê²½ë¡œ
    BEST_PT_FILE = "/data/danielsohn0827000/uni-dthon-project/best.pt"
    
    # ëª¨ë¸ ë¡œë“œ
    model = QueryBasedDetector(best_pt_path=BEST_PT_FILE)
    
    # ì„ì˜ì˜ í…ŒìŠ¤íŠ¸ ì´ë¯¸ì§€ì™€ ì§ˆë¬¸ìœ¼ë¡œ í…ŒìŠ¤íŠ¸
    test_img = "/data/danielsohn0827000/unid/open/test/images/MI2_240819_TY1_0012_3.jpg" # í…ŒìŠ¤íŠ¸ ì´ë¯¸ì§€ ê²½ë¡œë¡œ ìˆ˜ì •
    test_query = "ìœ ê°€ ë° ë‚˜í”„íƒ€ ê°€ê²© êº¾ì€ì„ í˜•"
    
    print(f"Test Query: {test_query}")
    bbox_xywh = model.predict(test_img, test_query)
    
    print(f"Predicted BBox [x, y, w, h]: {bbox_xywh}")