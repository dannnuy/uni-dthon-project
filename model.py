# íŒŒì¼ëª…: model.py
# (ìˆ˜ì • ì™„ë£Œëœ ìµœì¢… ë²„ì „)

import os
import torch
import easyocr
import numpy as np
from PIL import Image
from ultralytics import YOLO
from transformers import CLIPProcessor, CLIPModel

class QueryBasedDetector:
    def __init__(self, best_pt_path, device=None):
        """
        ì¶”ë¡ ì— í•„ìš”í•œ ëª¨ë“  ëª¨ë¸(YOLO, CLIP, OCR)ì„
        í•œ ë²ˆë§Œ ë¡œë“œí•˜ì—¬ í´ë˜ìŠ¤ì— ì €ì¥í•©ë‹ˆë‹¤.
        
        Args:
            best_pt_path (str): train.pyë¡œ í•™ìŠµì‹œí‚¨ 'best.pt' íŒŒì¼ ê²½ë¡œ
            device (str, optional): 'cuda' ë˜ëŠ” 'cpu'. Noneì´ë©´ ìë™ ê°ì§€.
        """
        
        if device is None:
            self.device = "cuda" if torch.cuda.is_available() else "cpu"
        else:
            self.device = device
            
        print(f"Using device: {self.device}")

        # 1. (Stage 1) YOLO íƒì§€ê¸° ë¡œë“œ
        self.detector = YOLO(best_pt_path).to(self.device)
        print(f"YOLO detector loaded from: {best_pt_path}")

        # 2. (Stage 2) CLIP ë§¤ì¹­ê¸° ë¡œë“œ
        clip_model_name = "openai/clip-vit-base-patch32"
        self.clip_model = CLIPModel.from_pretrained(clip_model_name).to(self.device)
        self.clip_processor = CLIPProcessor.from_pretrained(clip_model_name)
        print(f"CLIP model loaded: {clip_model_name}")

        # 3. (Stage 2) OCR ë¡œë“œ (ìº¡ì…˜ ì¶”ì¶œìš©)
        self.ocr_reader = easyocr.Reader(['ko', 'en'], gpu=(self.device == "cuda"))
        print("EasyOCR loaded.")

        # 4. ìŠ¤ì½”ì–´ë§ ê°€ì¤‘ì¹˜ (ì‹¤í—˜ì ìœ¼ë¡œ ì¡°ì ˆ)
        self.caption_weight = 0.7
        self.visual_weight = 0.3

    @torch.no_grad() # ì¶”ë¡  ëª¨ë“œì—ì„œëŠ” ê·¸ë˜ë””ì–¸íŠ¸ ê³„ì‚° ë¹„í™œì„±í™”
    def predict(self, image_path, query_text):
        """
        í•˜ë‚˜ì˜ ì´ë¯¸ì§€ì™€ ì§ˆë¬¸(query)ì„ ë°›ì•„
        ê°€ì¥ ì ìˆ˜ê°€ ë†’ì€ ê°ì²´ì˜ [x, y, w, h]ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.
        """
        
        try:
            image_pil = Image.open(image_path).convert("RGB")
        except Exception as e:
            print(f"Error opening image {image_path}: {e}")
            return [0, 0, 0, 0] 

        yolo_results = self.detector.predict(image_pil, verbose=False)
        candidate_boxes = yolo_results[0].boxes.xyxy.cpu().numpy() 

        if len(candidate_boxes) == 0:
            return [0, 0, 0, 0] 

        # 2-1. ì§ˆë¬¸(Query) í…ìŠ¤íŠ¸ë¥¼ CLIP í”¼ì²˜ë¡œ ë³€í™˜ (í•œ ë²ˆë§Œ)
        query_inputs = self.clip_processor(text=[query_text], return_tensors="pt", padding=True, truncation=True).to(self.device)
        query_features = self.clip_model.get_text_features(**query_inputs).detach()

        best_box = None
        highest_score = -float('inf')

        for box in candidate_boxes:
            x1, y1, x2, y2 = map(int, box)

            # --- ğŸ‘‡ 1. ìº¡ì…˜ 2ê°œ ì¶”ì¶œ (ìœ„/ì•„ë˜) ---
            caption_text_above = self._get_caption_text(image_pil, box, "above")
            caption_text_below = self._get_caption_text(image_pil, box, "below")
            # ---

            # 2-3. ì´ë¯¸ì§€ ì¡°ê°(patch) ì¶”ì¶œ
            patch_img = image_pil.crop((x1, y1, x2, y2))

            # --- ğŸ‘‡ 2. ì¸ì½”ë”© ë° ì ìˆ˜ ê³„ì‚° ë¡œì§ (ìˆ˜ì •ë¨) ---
            
            # (a) ì´ë¯¸ì§€ ì¸ì½”ë”©
            image_inputs = self.clip_processor(images=[patch_img], return_tensors="pt").to(self.device)
            visual_features = self.clip_model.get_image_features(image_inputs.pixel_values).detach()
            
            # (b) í…ìŠ¤íŠ¸ ì¸ì½”ë”© (ìœ„/ì•„ë˜)
            text_inputs = self.clip_processor(
                text=[caption_text_above, caption_text_below], # í…ìŠ¤íŠ¸ 2ê°œë¥¼ ë¦¬ìŠ¤íŠ¸ë¡œ ì „ë‹¬
                return_tensors="pt", 
                padding=True,
                truncation=True
            ).to(self.device)
            text_features = self.clip_model.get_text_features(text_inputs.input_ids, text_inputs.attention_mask).detach()
            caption_features_above = text_features[0] # ìœ„ ìº¡ì…˜ ë²¡í„°
            caption_features_below = text_features[1] # ì•„ë˜ ìº¡ì…˜ ë²¡í„°

            # 2-5. (Score) ì§ˆë¬¸(Query)ê³¼ì˜ ìœ ì‚¬ë„ ê³„ì‚°
            
            # (a) ì´ë¯¸ì§€ ì ìˆ˜ (ê³µí†µ)
            score_visual = torch.nn.functional.cosine_similarity(query_features, visual_features)

            # (b) í…ìŠ¤íŠ¸ ì ìˆ˜ (ìœ„/ì•„ë˜ ì¤‘ 'ìµœê³ ì 'ì„ ì„ íƒ)
            score_caption_above = torch.nn.functional.cosine_similarity(query_features, caption_features_above)
            score_caption_below = torch.nn.functional.cosine_similarity(query_features, caption_features_below)
            score_caption = max(score_caption_above, score_caption_below) # ğŸ‘ˆ ë‘˜ ì¤‘ ë†’ì€ ì ìˆ˜ë¥¼ ì‚¬ìš©

            final_score = (self.caption_weight * score_caption) + (self.visual_weight * score_visual)
            # --- ğŸ‘† ---
            
            if final_score > highest_score:
                highest_score = final_score
                best_box = box 

        # 3. í¬ë§· ë³€í™˜ ë° ë°˜í™˜
        if best_box is not None:
            x1, y1, x2, y2 = best_box
            pred_x = x1
            pred_y = y1
            pred_w = x2 - x1
            pred_h = y2 - y1
            return [float(pred_x), float(pred_y), float(pred_w), float(pred_h)]
        else:
            return [0, 0, 0, 0] 

    # --- ğŸ‘‡ 3. ì´ í•¨ìˆ˜ ì „ì²´ë¥¼ êµì²´í•´ì•¼ í•¨ ---
    def _get_caption_text(self, image_pil, box, position="below", margin_px=50):
        """
        (Helper) BBoxì˜ 'ìœ„(above)' ë˜ëŠ” 'ì•„ë˜(below)'ì—ì„œ í…ìŠ¤íŠ¸ë¥¼ OCRë¡œ ì¶”ì¶œ
        """
        try:
            x1, y1, x2, y2 = map(int, box)
            img_width, img_height = image_pil.size
            
            cap_x1 = max(0, x1)
            cap_x2 = min(x2, img_width)
            
            cap_y1, cap_y2 = 0, 0

            if position == "above":
                cap_y2 = max(0, y1)            # ìº¡ì…˜ ì˜ì—­ ë = BBoxì˜ ì²œì¥
                cap_y1 = max(0, y1 - margin_px)   # ìº¡ì…˜ ì˜ì—­ ì‹œì‘ = BBox ì²œì¥ - 50px
            else: # "below" (default)
                cap_y1 = min(y2, img_height)      # ìº¡ì…˜ ì˜ì—­ ì‹œì‘ = BBoxì˜ ë°”ë‹¥
                cap_y2 = min(y2 + margin_px, img_height) # ìº¡ì…˜ ì˜ì—­ ë = BBox ë°”ë‹¥ + 50px

            if cap_x1 >= cap_x2 or cap_y1 >= cap_y2:
                return "" # ì˜ì—­ì´ ì—†ìŒ

            caption_zone_img = image_pil.crop((cap_x1, cap_y1, cap_x2, cap_y2))
            
            ocr_results = self.ocr_reader.readtext(np.array(caption_zone_img), detail=0)
            return " ".join(ocr_results)
        except Exception as e:
            # OCR ì—ëŸ¬ê°€ ë‚˜ë„ ë¹ˆ ë¬¸ìì—´ì„ ë°˜í™˜í•˜ì—¬ ë©”ì¸ ë¡œì§ì´ ë©ˆì¶”ì§€ ì•Šê²Œ í•¨
            # print(f"Warning: OCR failed for box {box} ({position}). Error: {e}")
            return ""
    # --- ğŸ‘† ---

# ---
# ì´ íŒŒì¼ì´ ì§ì ‘ ì‹¤í–‰ë  ë•Œ (í…ŒìŠ¤íŠ¸ìš©)
if __name__ == "__main__":
    # train.pyì—ì„œ ìƒì„±ëœ 'best.pt' ê²½ë¡œ
    BEST_PT_FILE = "/data/danielsohn0827000/uni-dthon-project/best.pt"
    
    # ëª¨ë¸ ë¡œë“œ
    model = QueryBasedDetector(best_pt_path=BEST_PT_FILE)
    
    # ì„ì˜ì˜ í…ŒìŠ¤íŠ¸ ì´ë¯¸ì§€ì™€ ì§ˆë¬¸ìœ¼ë¡œ í…ŒìŠ¤íŠ¸
    test_img = "/data/danielsohn0827000/unid/open/test/images/MI2_240819_TY1_0012_3.jpg" # í…ŒìŠ¤íŠ¸ ì´ë¯¸ì§€ ê²½ë¡œë¡œ ìˆ˜ì •
    test_query = "ìœ ê°€ ë° ë‚˜í”„íƒ€ ê°€ê²© êº¾ì€ì„ í˜•"
    
    print(f"Test Query: {test_query}")
    bbox_xywh = model.predict(test_img, test_query)
    
    print(f"Predicted BBox [x, y, w, h]: {bbox_xywh}")